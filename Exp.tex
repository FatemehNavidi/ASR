
  \documentclass[12pt,epsfig,graphicx,amsmath,color]{article}

\usepackage[]{algorithm2e}
   \providecommand{\tabularnewline}{\\}
	\usepackage{amsmath}
	\usepackage{graphicx}
	\usepackage{color}
   \renewcommand{\baselinestretch}{1.5}
   \setlength{\oddsidemargin}{0cm}
   \setlength{\textwidth}{16.4cm}
   \setlength{\topmargin}{0in}
   \setlength{\textheight}{23cm}
   \setlength{\headsep}{0in}
   \setlength{\headheight}{0pt}
	\newcommand{\ignore}[1]{}
   %
   \nonstopmode
   %
   \pagestyle{plain}
   %
   \newcommand{\mb}[1]{\mbox{\boldmath $#1$}}
   \newcommand{\nbox}[1]{\ \ \mbox{#1} \ \ }
   %
   \usepackage{bm}
\title{Experiments}
\begin{document}
Assume that we are at state $(E,H)$. Now, we discuss the algorithms for choosing $e$ for both Optimal Decision Tree problem and Multiple Intent Re-Ranking Problem.
\section{Optimal Decision Tree}
\subsection{Greedy Algorithm}
This algorithm is based on the classic greedy objective discussed in \cite{KPB99}, \cite{D01}, \cite{AH12}, \cite{CPRAM11}, \cite{GB09}. At each step, the greedy algorithm will pick the element which makes the decision tree as balanced as possible. So we choose the element $e$ such that: 
$$e = argmax_{e\in U\setminus E}\min(|\{i|i\in H, r_i(e)=1\}|,|\{i|i\in H, r_i(e)=0\}|)$$
Then we update $E$ and $H$ as follows:
$$E\leftarrow E\cup \{e\}$$
$$H\leftarrow H \setminus \{i|r_i(e)=0\}$$
\subsection{Static Algorithm}
The static algorithm that we are using is from a paper by Azar and Gamzu paper $\cite{AG11}$. By the term ``static'' we are referring to this fact that the algorithm is not dependent on the elements' feedback. Let $T_e$ be the set of scenarios that have feedback 1 on test $e$. Now, we define $T_e(i)=T_e$ if and only if $r_i(e)=0$ (or equivalently $i\notin T_e$) and $T_e(i)=[m]\setminus T_e$ if and only if  $r_i(e)=1$ (or equivalently $i\in T_e$) . Then we define $f_i(S) =|\bigcup_{e\in S}T_i(e)|\cdot\frac{1}{m-1}$. In static algorithm, we choose element $e$ such that:
$$e = argmax_{e\in U\setminus E} \sum\limits_{i\in H} p_i \cdot \frac{f_i(e\cup E)-f_i(E)}{1-f_i(E)} $$
Then we update $E$ and $H$ as follows:
$$E\leftarrow E\cup \{e\}$$
$$H\leftarrow H \setminus \{i|f_i(E)=1\}$$
\subsection{Ad-Static Algorithm}
This is a modified version of the static algorithm discussed above. Basically everything is the same except for updating the set $H$:
$$H\leftarrow H \setminus \{i|r_i(e)=0\}$$
\subsection{ML-Based Algorithm}
This is a ``machine learning'' algorithm that for a parameter $K$, uses $k$-Means \cite{arthur2007k} to \textit{a priori} partition $U$ into $K$ clusters. Each cluster $c_j, j\in{}[1,K]$ is initially given a weight $w_{c_j} = 1$.
To choose the next element $e\in{}U\setminus{}E$, a cluster $j\in{}[1,K]$ is
first chosen by sampling non-uniformly according to $w_{c_j}$.
Next, an element $e\in{}c_j$ is chosen uniformly at random.
If $r_{i^*}(e)=1$, the $c_j$ is rewarded by setting $w_{c_j} =
2 w_{c_j}$, else $c_j$ is penalized by setting $w_{c_j} = 0.5 w_{c_j}$. Again based on $e$'s feedback we update $E$ and $H$:
$$E\leftarrow E\cup \{e\}$$
$$H\leftarrow H \setminus \{i|r_i(e)=0\}$$

\subsection{Adaptive Submodular Ranking Algorithm}
Lets define $L_e(H) = argmin(|\{i|i\in H, r_i(e)=1\}|,|\{i|i\in H, r_i(e)=0\}|)$, which means that $L_e(H)$ corresponds to the lighter branch of the algorithm for a test $e$. More specifically for any test $e$ and set of compatible scenarios $H$ if test $e$ has outcome 1 for $H_1\subseteq H$ and 0 for $H\setminus H_1$, then $L_e(H)$ will be equal to $H_1$ if and only if $|H_1|<|H|/2$ and will be equal to $H\setminus H_1$ otherwise. $f_i$s have the same definition as before. Now, our algorithm at any state $(E.H)$ choose element $e$ such that:
$$e = argmax_{e\in U\setminus E}\left( \sum_{j\in L_e(H)} p_j \,\,+\,\, \sum\limits_{i\in H} p_i \cdot \frac{f_i(e\cup E)-f_i(E)}{1-f_i(E)} \right)$$

\section{Multiple Intents Re-Ranking}
\subsection{Greedy Algorithm}
At each step, the greedy algorithm will pick the element with the maximum probability of having feedback 1. In other words, we choose the element $e$ such that: 
$$e = argmax_{e\in U\setminus E}\sum_{i\in H: r_i(e)=1}p_i$$
Then we update $E$ and $H$ as follows:
$$E\leftarrow E\cup \{e\}$$
$$H\leftarrow H \setminus \{i|r_i(e)=0\}$$
\subsection{Static Algorithm}
We are going to use the algorithm in \cite{AGY09} for the general case. Let $S_i$ be the set of relevant results for user (scenario) $i$.

\begin{algorithm}[H]
\While{$U\setminus E \neq \emptyset$}{
	\For{$i\in [m]$}{
    	\If{$|S_i\cap E|<K_i$}{
        $\rho(i)=\frac{1}{K_i-|S_i\cap E|}$}
        \Else{
        $\rho(i) = 0$}
    }
    $e \gets argmax_{e\in U\setminus E}\sum_{i:e\in S_i}\rho(i)$\\
    $E\gets E\cup \{e\}$
}
\end{algorithm}

\ignore{that we have non-increasing weight. At each step we need to solve the relaxation of the following integer programming:
\begin{gather*}
\begin{aligned}
&\min\sum_{i\in H}y_i \\
y_i&\geq \sum_{e: r_i(e)=1}x_{\pi(e)}&\forall i\in H, \forall \pi\in \Pi\\
\sum_{e\in E'}x_e&\geq (|E'|+{|E'|}^2)/2&\forall E'\subseteq U\\
y_i&\in R^+&\forall i\in H\\
x_e&\in N&\forall e\in E
\end{aligned}
\end{gather*}}

\subsection{Ad-Static Algorithm}
This is a modified version of the static algorithm discussed above. Basically everything is the same except for updating the set $H$:
$$H\leftarrow H \setminus \{i|r_i(e)=0\}$$

\subsection{ML-Based Algorithm}
This is exactly the same as ML-based algorithm for Optimal Decision Tree problem.

\subsection{Adaptive Submodular Ranking Algorithm}
Lets $L_e(H)$ have the same definition as before (i.e. $L_e(H) = argmin(|\{i|i\in H, r_i(e)=1\}|,|\{i|i\in H, r_i(e)=0\}|)$). We define $f_i(S) = \min(|S\cap S_i|,K_i)/K_i$. Now, at any state $(E.H)$ choose element $e$ such that:
$$e = argmax_{e\in U\setminus E}\left( \sum_{j\in L_e(H)} p_j \,\,+\,\, \sum\limits_{i\in H} p_i \cdot \frac{f_i(e\cup E)-f_i(E)}{1-f_i(E)} \right)$$



\bibliographystyle{plain}
\bibliography{adrank.bib}
\end{document}